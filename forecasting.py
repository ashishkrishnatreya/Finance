# -*- coding: utf-8 -*-
"""FORECASTING.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/142DgRWXAlDhHwxr7ouz6HdP49IBcx8FF
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import datetime
from sklearn.svm import SVC
svclassifier = SVC(kernel='linear')

from google.colab import files
uploaded = files.upload()

import io
df2 = pd.read_csv(io.BytesIO(uploaded['Nifty 50 Historical Data.csv']))
# Dataset is now stored in a Pandas Dataframe
df2.shape

df2['Vol.'] = [x[:-1] for x in df2['Vol.']]
df2.head()

#checking the missing data
df2.isnull().sum()

import locale
locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
for i in range(len(df2)):
  df2.Price[i] = locale.atof(str(df2.Price[i]))
  df2.Open[i] = locale.atof(str(df2.Open[i]))
  df2.High[i] = locale.atof(str(df2.High[i]))
  df2.Low[i] = locale.atof(str(df2.Low[i]))
df2

# Commented out IPython magic to ensure Python compatibility.
!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz
!tar -xzvf ta-lib-0.4.0-src.tar.gz
# %cd ta-lib
!./configure --prefix=/usr
!make
!make install
!pip install Ta-Lib
import talib

df2 = df2[['Price', 'Open', 'High', 'Low', 'Vol.']]

df = df2.copy()

stock_high = df['High']
stock_low = df['Low']
stock_open = df['Open']
stock_close = df['Price']
stock_volume = df['Vol.']

from matplotlib import pyplot as plt
plt.hist(stock_close)

df2['SMA(10)'] = df.Price.rolling(10).mean()
df2['SMA(20)'] = df.Price.rolling(20).mean()
df2['SMA(50)'] = df.Price.rolling(50).mean()
df2['SMA(100)'] = df.Price.rolling(100).mean()
df2

df2['ema_10'] = df2.Price.ewm(span=10).mean().fillna(0)
df2['ema_20'] = df2.Price.ewm(span=20).mean().fillna(0)
df2['ema_50'] = df2.Price.ewm(span=50).mean().fillna(0)
df2['ema_100'] = df2.Price.ewm(span=100).mean().fillna(0)
df2

df2.High = df2.High.apply(float)
type(df2.High[0])
df2.Low = df2.Low.apply(float)
type(df2.Low[0])
df2.Price = df2.Price.apply(float)
type(df2.Price[0])

df2['ATR'] = talib.ATR(df2['High'].values,df2['Low'].values,df2['Price'].values,timeperiod=14)
df2

tp = (df2['High']+df2['Low']+df2['Price'])/3
ma = tp/20
md = (tp-ma)/20
df2['CCI'] = (tp-ma)/(0.015*md)
df2

df2['ROC'] = ((df2['Price']-df2['Price'].shift(12))/(df2['Price'].shift(12)))*100
df2

df2['rsi'] = talib.RSI(df2.Price.values,timeperiod=14)
df2

df2['William %R'] = talib.WILLR(df2.High.values,df2.Low.values ,df2.Price.values, timeperiod=14)
df2

df2['SO%K'] = ((df2['Price']-df2['Low'])/(df2['High']-df2['Low']))
df2

print('Total dataset has{} samples, and {} features.'.format(df2.shape[0],df2.shape[1]))

df2['pred_price'] = np.where(df2['Price'].shift(-1) > df2['Price'],1,0)
df2['pred_price'].unique()
df2

df2 = df2.dropna()
print('Total dataset has{} samples, and {} features.'.format(df2.shape[0],df2.shape[1]))

df2.index.name = 'S.NO'
df2

y = df2['pred_price']
x = df2.drop(columns = ['pred_price'])
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)
x_train

plt.figure(figsize=(10,6))
ax = x_train.plot(grid=True,figsize=(20,8))
x_test.plot(ax=ax,grid=True)
plt.legend(['train','test'])
plt.show

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range = (0,1))
x_train_scaled = scaler.fit_transform(x_train)
x_train_scaled

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

import time
dict_classifiers = {
    "Logistics Regression": LogisticRegression(solver='lbfgs', max_iter=5000),
    "Nearest Neighbors":  KNeighborsClassifier(),
    "Support Vector Machine": SVC(gamma = 'auto'),
    "naive bayes": GaussianNB()
}

no_classifiers = len(dict_classifiers.keys())
def batch_classify(x_train_scaled,y_train, verbose = True):
  df2_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,3)),columns=['classifiers','train_score','training_time'])
  count = 0
  for key,classifier in dict_classifiers.items():
    t_start = time.process_time()
    classifier.fit(x_train_scaled,y_train)
    t_end = time.process_time()
    t_diff = t_end - t_start
    train_score = classifier.score(x_train_scaled,y_train)
    df2_results.loc[count,'classifier'] = key
    df2_results.loc[count,'train_score'] = train_score
    df2_results.loc[count,'training_time'] = t_diff
    if verbose:
      print("trained{c}in {f:2f} s".format(c=key, f=t_diff))
      count+=1
    
  return df2_results

df2_results = batch_classify(x_train_scaled,y_train)
print(df2_results.sort_values(by='train_score', ascending=True))

results_df2 = df2_results.sort_values(by=['train_score'], ascending=False)
results_df2

# log_reg = LogisticRegression(solver='lbfgs', max_iter=5000)
# log_reg.fit(x_train_scaled, y_train)
# log_reg
svc_1 = SVC(gamma = 'auto')
svc_1.fit(x_train_scaled, y_train)
svc_1
# KN_1 = KNeighborsClassifier()
# KN_1.fit(x_train_scaled, y_train)
# KN_1
# GB_1 = GaussianNB()
# GB_1.fit(x_train_scaled, y_train)
# GB_1
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report 
predictions1 = svc_1.predict(x_test)
#predictions2 = KN_1.predict(x_test)
# predictions3 = GB_1.predict(x_test)
#predictions4 = log_reg.predict(x_test)
print(accuracy_score(y_test, predictions1))
# print(accuracy_score(y_test, predictions2))
# print(accuracy_score(y_test, predictions3))
# print(accuracy_score(y_test, predictions4))
print("Confusion Matrix:")
print(confusion_matrix(y_test, predictions1))
# print("Confusion Matrix:")
# print(confusion_matrix(y_test, predictions2))
# print("Confusion Matrix:")
# print(confusion_matrix(y_test, predictions3))
# print("Classification Report")
print(classification_report(y_test, predictions1))